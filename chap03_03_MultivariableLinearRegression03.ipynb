{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chap03_03_MultivariableLinearRegression03.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPanuojHx4fr372lU3S3gEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeuklyoungKo/toStartDeepLearningWithPyTorch/blob/main/chap03_03_MultivariableLinearRegression03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwHz-Gy2yPhg",
        "outputId": "a75e1854-8407-4409-a9e9-c22c7e4d94c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9740bf5750>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  80], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "print(\"shape:\",x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZhbgapgymfu",
        "outputId": "ac3e890e-8a34-4370-882e-7fad58b51193"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([5, 3]) torch.Size([5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치와 편향 선언\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "KNpcXkO6yswa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD([W, b], lr=1e-5)"
      ],
      "metadata": {
        "id": "k3kNvhe5zBI1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 200\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
        "    hypothesis = x_train.matmul(W) + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNDOi8XizJa5",
        "outputId": "5844ec4f-169a-48d1-d2ed-7e7e694a6c7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/200 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch    1/200 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
            "Epoch    2/200 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]) Cost: 3069.590088\n",
            "Epoch    3/200 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
            "Epoch    4/200 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n",
            "Epoch    5/200 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
            "Epoch    6/200 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n",
            "Epoch    7/200 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n",
            "Epoch    8/200 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n",
            "Epoch    9/200 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n",
            "Epoch   10/200 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n",
            "Epoch   11/200 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
            "Epoch   12/200 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n",
            "Epoch   13/200 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n",
            "Epoch   14/200 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n",
            "Epoch   15/200 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n",
            "Epoch   16/200 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n",
            "Epoch   17/200 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n",
            "Epoch   18/200 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n",
            "Epoch   19/200 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n",
            "Epoch   20/200 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n",
            "Epoch   21/200 hypothesis: tensor([154.0543, 185.1140, 175.7470, 198.6150, 141.2166]) Cost: 5.954494\n",
            "Epoch   22/200 hypothesis: tensor([154.0546, 185.1143, 175.7486, 198.6150, 141.2171]) Cost: 5.951927\n",
            "Epoch   23/200 hypothesis: tensor([154.0547, 185.1143, 175.7500, 198.6147, 141.2173]) Cost: 5.949370\n",
            "Epoch   24/200 hypothesis: tensor([154.0546, 185.1142, 175.7512, 198.6143, 141.2175]) Cost: 5.946794\n",
            "Epoch   25/200 hypothesis: tensor([154.0546, 185.1140, 175.7523, 198.6138, 141.2177]) Cost: 5.944210\n",
            "Epoch   26/200 hypothesis: tensor([154.0545, 185.1137, 175.7535, 198.6133, 141.2178]) Cost: 5.941589\n",
            "Epoch   27/200 hypothesis: tensor([154.0543, 185.1135, 175.7545, 198.6127, 141.2178]) Cost: 5.939054\n",
            "Epoch   28/200 hypothesis: tensor([154.0542, 185.1132, 175.7556, 198.6122, 141.2179]) Cost: 5.936491\n",
            "Epoch   29/200 hypothesis: tensor([154.0541, 185.1129, 175.7567, 198.6116, 141.2180]) Cost: 5.933901\n",
            "Epoch   30/200 hypothesis: tensor([154.0539, 185.1126, 175.7578, 198.6110, 141.2180]) Cost: 5.931320\n",
            "Epoch   31/200 hypothesis: tensor([154.0538, 185.1123, 175.7588, 198.6104, 141.2181]) Cost: 5.928727\n",
            "Epoch   32/200 hypothesis: tensor([154.0536, 185.1120, 175.7599, 198.6098, 141.2181]) Cost: 5.926240\n",
            "Epoch   33/200 hypothesis: tensor([154.0535, 185.1118, 175.7610, 198.6093, 141.2182]) Cost: 5.923646\n",
            "Epoch   34/200 hypothesis: tensor([154.0533, 185.1115, 175.7620, 198.6086, 141.2183]) Cost: 5.921031\n",
            "Epoch   35/200 hypothesis: tensor([154.0532, 185.1112, 175.7631, 198.6081, 141.2183]) Cost: 5.918510\n",
            "Epoch   36/200 hypothesis: tensor([154.0530, 185.1109, 175.7641, 198.6075, 141.2184]) Cost: 5.915939\n",
            "Epoch   37/200 hypothesis: tensor([154.0529, 185.1106, 175.7652, 198.6069, 141.2184]) Cost: 5.913393\n",
            "Epoch   38/200 hypothesis: tensor([154.0527, 185.1103, 175.7662, 198.6063, 141.2185]) Cost: 5.910825\n",
            "Epoch   39/200 hypothesis: tensor([154.0526, 185.1100, 175.7673, 198.6057, 141.2185]) Cost: 5.908256\n",
            "Epoch   40/200 hypothesis: tensor([154.0524, 185.1097, 175.7683, 198.6051, 141.2186]) Cost: 5.905738\n",
            "Epoch   41/200 hypothesis: tensor([154.0523, 185.1094, 175.7694, 198.6045, 141.2187]) Cost: 5.903165\n",
            "Epoch   42/200 hypothesis: tensor([154.0521, 185.1091, 175.7705, 198.6039, 141.2187]) Cost: 5.900594\n",
            "Epoch   43/200 hypothesis: tensor([154.0520, 185.1089, 175.7715, 198.6034, 141.2188]) Cost: 5.898070\n",
            "Epoch   44/200 hypothesis: tensor([154.0518, 185.1086, 175.7726, 198.6028, 141.2188]) Cost: 5.895528\n",
            "Epoch   45/200 hypothesis: tensor([154.0517, 185.1083, 175.7736, 198.6022, 141.2189]) Cost: 5.892972\n",
            "Epoch   46/200 hypothesis: tensor([154.0515, 185.1080, 175.7747, 198.6016, 141.2189]) Cost: 5.890395\n",
            "Epoch   47/200 hypothesis: tensor([154.0514, 185.1077, 175.7757, 198.6010, 141.2190]) Cost: 5.887882\n",
            "Epoch   48/200 hypothesis: tensor([154.0512, 185.1074, 175.7768, 198.6004, 141.2191]) Cost: 5.885315\n",
            "Epoch   49/200 hypothesis: tensor([154.0511, 185.1071, 175.7778, 198.5998, 141.2191]) Cost: 5.882764\n",
            "Epoch   50/200 hypothesis: tensor([154.0509, 185.1068, 175.7789, 198.5993, 141.2192]) Cost: 5.880200\n",
            "Epoch   51/200 hypothesis: tensor([154.0508, 185.1065, 175.7800, 198.5987, 141.2192]) Cost: 5.877647\n",
            "Epoch   52/200 hypothesis: tensor([154.0506, 185.1062, 175.7810, 198.5981, 141.2193]) Cost: 5.875116\n",
            "Epoch   53/200 hypothesis: tensor([154.0504, 185.1060, 175.7821, 198.5975, 141.2193]) Cost: 5.872606\n",
            "Epoch   54/200 hypothesis: tensor([154.0503, 185.1057, 175.7831, 198.5969, 141.2194]) Cost: 5.870055\n",
            "Epoch   55/200 hypothesis: tensor([154.0502, 185.1054, 175.7842, 198.5963, 141.2195]) Cost: 5.867533\n",
            "Epoch   56/200 hypothesis: tensor([154.0500, 185.1051, 175.7852, 198.5957, 141.2195]) Cost: 5.864987\n",
            "Epoch   57/200 hypothesis: tensor([154.0498, 185.1048, 175.7863, 198.5952, 141.2196]) Cost: 5.862448\n",
            "Epoch   58/200 hypothesis: tensor([154.0497, 185.1045, 175.7873, 198.5946, 141.2196]) Cost: 5.859921\n",
            "Epoch   59/200 hypothesis: tensor([154.0495, 185.1042, 175.7884, 198.5940, 141.2197]) Cost: 5.857368\n",
            "Epoch   60/200 hypothesis: tensor([154.0494, 185.1039, 175.7894, 198.5934, 141.2198]) Cost: 5.854844\n",
            "Epoch   61/200 hypothesis: tensor([154.0492, 185.1037, 175.7905, 198.5928, 141.2198]) Cost: 5.852332\n",
            "Epoch   62/200 hypothesis: tensor([154.0491, 185.1034, 175.7915, 198.5922, 141.2199]) Cost: 5.849802\n",
            "Epoch   63/200 hypothesis: tensor([154.0489, 185.1031, 175.7926, 198.5916, 141.2199]) Cost: 5.847257\n",
            "Epoch   64/200 hypothesis: tensor([154.0488, 185.1028, 175.7936, 198.5911, 141.2200]) Cost: 5.844723\n",
            "Epoch   65/200 hypothesis: tensor([154.0486, 185.1025, 175.7947, 198.5905, 141.2201]) Cost: 5.842195\n",
            "Epoch   66/200 hypothesis: tensor([154.0485, 185.1022, 175.7957, 198.5899, 141.2201]) Cost: 5.839685\n",
            "Epoch   67/200 hypothesis: tensor([154.0483, 185.1019, 175.7968, 198.5893, 141.2202]) Cost: 5.837132\n",
            "Epoch   68/200 hypothesis: tensor([154.0482, 185.1016, 175.7978, 198.5887, 141.2202]) Cost: 5.834631\n",
            "Epoch   69/200 hypothesis: tensor([154.0480, 185.1013, 175.7989, 198.5882, 141.2203]) Cost: 5.832149\n",
            "Epoch   70/200 hypothesis: tensor([154.0479, 185.1011, 175.7999, 198.5876, 141.2203]) Cost: 5.829581\n",
            "Epoch   71/200 hypothesis: tensor([154.0477, 185.1008, 175.8010, 198.5870, 141.2204]) Cost: 5.827082\n",
            "Epoch   72/200 hypothesis: tensor([154.0476, 185.1005, 175.8020, 198.5864, 141.2205]) Cost: 5.824571\n",
            "Epoch   73/200 hypothesis: tensor([154.0474, 185.1002, 175.8031, 198.5858, 141.2205]) Cost: 5.822036\n",
            "Epoch   74/200 hypothesis: tensor([154.0473, 185.0999, 175.8041, 198.5852, 141.2206]) Cost: 5.819564\n",
            "Epoch   75/200 hypothesis: tensor([154.0471, 185.0996, 175.8051, 198.5846, 141.2206]) Cost: 5.817031\n",
            "Epoch   76/200 hypothesis: tensor([154.0470, 185.0993, 175.8062, 198.5840, 141.2207]) Cost: 5.814490\n",
            "Epoch   77/200 hypothesis: tensor([154.0468, 185.0991, 175.8073, 198.5835, 141.2207]) Cost: 5.811966\n",
            "Epoch   78/200 hypothesis: tensor([154.0466, 185.0988, 175.8083, 198.5829, 141.2208]) Cost: 5.809479\n",
            "Epoch   79/200 hypothesis: tensor([154.0465, 185.0985, 175.8093, 198.5823, 141.2209]) Cost: 5.806978\n",
            "Epoch   80/200 hypothesis: tensor([154.0464, 185.0982, 175.8104, 198.5817, 141.2209]) Cost: 5.804478\n",
            "Epoch   81/200 hypothesis: tensor([154.0462, 185.0979, 175.8114, 198.5812, 141.2210]) Cost: 5.801940\n",
            "Epoch   82/200 hypothesis: tensor([154.0460, 185.0976, 175.8125, 198.5806, 141.2210]) Cost: 5.799462\n",
            "Epoch   83/200 hypothesis: tensor([154.0459, 185.0974, 175.8135, 198.5800, 141.2211]) Cost: 5.796945\n",
            "Epoch   84/200 hypothesis: tensor([154.0457, 185.0971, 175.8146, 198.5794, 141.2212]) Cost: 5.794450\n",
            "Epoch   85/200 hypothesis: tensor([154.0456, 185.0968, 175.8156, 198.5788, 141.2212]) Cost: 5.791949\n",
            "Epoch   86/200 hypothesis: tensor([154.0454, 185.0965, 175.8166, 198.5782, 141.2213]) Cost: 5.789460\n",
            "Epoch   87/200 hypothesis: tensor([154.0453, 185.0962, 175.8177, 198.5777, 141.2213]) Cost: 5.786945\n",
            "Epoch   88/200 hypothesis: tensor([154.0451, 185.0959, 175.8187, 198.5771, 141.2214]) Cost: 5.784457\n",
            "Epoch   89/200 hypothesis: tensor([154.0450, 185.0956, 175.8198, 198.5765, 141.2215]) Cost: 5.781941\n",
            "Epoch   90/200 hypothesis: tensor([154.0448, 185.0954, 175.8208, 198.5759, 141.2215]) Cost: 5.779487\n",
            "Epoch   91/200 hypothesis: tensor([154.0447, 185.0951, 175.8219, 198.5753, 141.2216]) Cost: 5.776975\n",
            "Epoch   92/200 hypothesis: tensor([154.0445, 185.0948, 175.8229, 198.5748, 141.2216]) Cost: 5.774490\n",
            "Epoch   93/200 hypothesis: tensor([154.0444, 185.0945, 175.8239, 198.5742, 141.2217]) Cost: 5.771988\n",
            "Epoch   94/200 hypothesis: tensor([154.0442, 185.0942, 175.8250, 198.5736, 141.2218]) Cost: 5.769504\n",
            "Epoch   95/200 hypothesis: tensor([154.0441, 185.0940, 175.8260, 198.5730, 141.2218]) Cost: 5.767023\n",
            "Epoch   96/200 hypothesis: tensor([154.0439, 185.0937, 175.8271, 198.5724, 141.2219]) Cost: 5.764544\n",
            "Epoch   97/200 hypothesis: tensor([154.0437, 185.0934, 175.8281, 198.5718, 141.2219]) Cost: 5.762030\n",
            "Epoch   98/200 hypothesis: tensor([154.0436, 185.0931, 175.8291, 198.5713, 141.2220]) Cost: 5.759563\n",
            "Epoch   99/200 hypothesis: tensor([154.0434, 185.0928, 175.8302, 198.5707, 141.2221]) Cost: 5.757066\n",
            "Epoch  100/200 hypothesis: tensor([154.0433, 185.0925, 175.8312, 198.5701, 141.2221]) Cost: 5.754573\n",
            "Epoch  101/200 hypothesis: tensor([154.0431, 185.0923, 175.8322, 198.5695, 141.2222]) Cost: 5.752121\n",
            "Epoch  102/200 hypothesis: tensor([154.0430, 185.0920, 175.8333, 198.5690, 141.2222]) Cost: 5.749629\n",
            "Epoch  103/200 hypothesis: tensor([154.0428, 185.0917, 175.8343, 198.5684, 141.2223]) Cost: 5.747150\n",
            "Epoch  104/200 hypothesis: tensor([154.0427, 185.0914, 175.8354, 198.5678, 141.2224]) Cost: 5.744630\n",
            "Epoch  105/200 hypothesis: tensor([154.0425, 185.0911, 175.8364, 198.5672, 141.2224]) Cost: 5.742209\n",
            "Epoch  106/200 hypothesis: tensor([154.0424, 185.0908, 175.8374, 198.5667, 141.2225]) Cost: 5.739733\n",
            "Epoch  107/200 hypothesis: tensor([154.0422, 185.0905, 175.8385, 198.5661, 141.2225]) Cost: 5.737256\n",
            "Epoch  108/200 hypothesis: tensor([154.0421, 185.0903, 175.8395, 198.5655, 141.2226]) Cost: 5.734780\n",
            "Epoch  109/200 hypothesis: tensor([154.0419, 185.0900, 175.8405, 198.5649, 141.2227]) Cost: 5.732310\n",
            "Epoch  110/200 hypothesis: tensor([154.0417, 185.0897, 175.8416, 198.5643, 141.2227]) Cost: 5.729835\n",
            "Epoch  111/200 hypothesis: tensor([154.0416, 185.0894, 175.8426, 198.5638, 141.2228]) Cost: 5.727385\n",
            "Epoch  112/200 hypothesis: tensor([154.0415, 185.0892, 175.8437, 198.5632, 141.2229]) Cost: 5.724895\n",
            "Epoch  113/200 hypothesis: tensor([154.0413, 185.0889, 175.8447, 198.5626, 141.2229]) Cost: 5.722435\n",
            "Epoch  114/200 hypothesis: tensor([154.0411, 185.0886, 175.8457, 198.5620, 141.2230]) Cost: 5.719935\n",
            "Epoch  115/200 hypothesis: tensor([154.0410, 185.0883, 175.8468, 198.5614, 141.2230]) Cost: 5.717507\n",
            "Epoch  116/200 hypothesis: tensor([154.0408, 185.0880, 175.8478, 198.5609, 141.2231]) Cost: 5.715061\n",
            "Epoch  117/200 hypothesis: tensor([154.0407, 185.0877, 175.8488, 198.5603, 141.2232]) Cost: 5.712591\n",
            "Epoch  118/200 hypothesis: tensor([154.0405, 185.0875, 175.8499, 198.5597, 141.2232]) Cost: 5.710109\n",
            "Epoch  119/200 hypothesis: tensor([154.0404, 185.0872, 175.8509, 198.5591, 141.2233]) Cost: 5.707640\n",
            "Epoch  120/200 hypothesis: tensor([154.0402, 185.0869, 175.8519, 198.5585, 141.2233]) Cost: 5.705184\n",
            "Epoch  121/200 hypothesis: tensor([154.0401, 185.0866, 175.8530, 198.5580, 141.2234]) Cost: 5.702760\n",
            "Epoch  122/200 hypothesis: tensor([154.0399, 185.0863, 175.8540, 198.5574, 141.2235]) Cost: 5.700261\n",
            "Epoch  123/200 hypothesis: tensor([154.0397, 185.0860, 175.8550, 198.5568, 141.2235]) Cost: 5.697819\n",
            "Epoch  124/200 hypothesis: tensor([154.0396, 185.0858, 175.8560, 198.5562, 141.2236]) Cost: 5.695369\n",
            "Epoch  125/200 hypothesis: tensor([154.0394, 185.0855, 175.8571, 198.5557, 141.2236]) Cost: 5.692929\n",
            "Epoch  126/200 hypothesis: tensor([154.0393, 185.0852, 175.8581, 198.5551, 141.2237]) Cost: 5.690452\n",
            "Epoch  127/200 hypothesis: tensor([154.0392, 185.0850, 175.8591, 198.5545, 141.2238]) Cost: 5.688024\n",
            "Epoch  128/200 hypothesis: tensor([154.0390, 185.0847, 175.8602, 198.5539, 141.2238]) Cost: 5.685557\n",
            "Epoch  129/200 hypothesis: tensor([154.0388, 185.0844, 175.8612, 198.5533, 141.2239]) Cost: 5.683081\n",
            "Epoch  130/200 hypothesis: tensor([154.0387, 185.0841, 175.8622, 198.5528, 141.2240]) Cost: 5.680676\n",
            "Epoch  131/200 hypothesis: tensor([154.0385, 185.0838, 175.8633, 198.5522, 141.2240]) Cost: 5.678251\n",
            "Epoch  132/200 hypothesis: tensor([154.0384, 185.0836, 175.8643, 198.5517, 141.2241]) Cost: 5.675793\n",
            "Epoch  133/200 hypothesis: tensor([154.0382, 185.0833, 175.8653, 198.5511, 141.2242]) Cost: 5.673342\n",
            "Epoch  134/200 hypothesis: tensor([154.0381, 185.0830, 175.8663, 198.5505, 141.2242]) Cost: 5.670938\n",
            "Epoch  135/200 hypothesis: tensor([154.0379, 185.0827, 175.8674, 198.5499, 141.2243]) Cost: 5.668463\n",
            "Epoch  136/200 hypothesis: tensor([154.0378, 185.0824, 175.8684, 198.5493, 141.2243]) Cost: 5.666020\n",
            "Epoch  137/200 hypothesis: tensor([154.0376, 185.0822, 175.8694, 198.5488, 141.2244]) Cost: 5.663587\n",
            "Epoch  138/200 hypothesis: tensor([154.0374, 185.0819, 175.8705, 198.5482, 141.2245]) Cost: 5.661154\n",
            "Epoch  139/200 hypothesis: tensor([154.0373, 185.0816, 175.8715, 198.5476, 141.2245]) Cost: 5.658710\n",
            "Epoch  140/200 hypothesis: tensor([154.0371, 185.0813, 175.8725, 198.5470, 141.2246]) Cost: 5.656279\n",
            "Epoch  141/200 hypothesis: tensor([154.0370, 185.0811, 175.8735, 198.5465, 141.2246]) Cost: 5.653854\n",
            "Epoch  142/200 hypothesis: tensor([154.0368, 185.0808, 175.8746, 198.5459, 141.2247]) Cost: 5.651419\n",
            "Epoch  143/200 hypothesis: tensor([154.0367, 185.0805, 175.8756, 198.5453, 141.2248]) Cost: 5.648982\n",
            "Epoch  144/200 hypothesis: tensor([154.0365, 185.0802, 175.8766, 198.5448, 141.2248]) Cost: 5.646577\n",
            "Epoch  145/200 hypothesis: tensor([154.0364, 185.0799, 175.8776, 198.5442, 141.2249]) Cost: 5.644108\n",
            "Epoch  146/200 hypothesis: tensor([154.0362, 185.0797, 175.8786, 198.5436, 141.2250]) Cost: 5.641737\n",
            "Epoch  147/200 hypothesis: tensor([154.0361, 185.0794, 175.8797, 198.5430, 141.2250]) Cost: 5.639285\n",
            "Epoch  148/200 hypothesis: tensor([154.0359, 185.0791, 175.8807, 198.5425, 141.2251]) Cost: 5.636886\n",
            "Epoch  149/200 hypothesis: tensor([154.0357, 185.0788, 175.8817, 198.5419, 141.2252]) Cost: 5.634415\n",
            "Epoch  150/200 hypothesis: tensor([154.0356, 185.0786, 175.8828, 198.5413, 141.2252]) Cost: 5.632010\n",
            "Epoch  151/200 hypothesis: tensor([154.0354, 185.0783, 175.8838, 198.5407, 141.2253]) Cost: 5.629580\n",
            "Epoch  152/200 hypothesis: tensor([154.0353, 185.0780, 175.8848, 198.5402, 141.2253]) Cost: 5.627147\n",
            "Epoch  153/200 hypothesis: tensor([154.0351, 185.0777, 175.8858, 198.5396, 141.2254]) Cost: 5.624751\n",
            "Epoch  154/200 hypothesis: tensor([154.0350, 185.0774, 175.8869, 198.5390, 141.2255]) Cost: 5.622303\n",
            "Epoch  155/200 hypothesis: tensor([154.0348, 185.0772, 175.8879, 198.5385, 141.2255]) Cost: 5.619937\n",
            "Epoch  156/200 hypothesis: tensor([154.0347, 185.0769, 175.8889, 198.5379, 141.2256]) Cost: 5.617486\n",
            "Epoch  157/200 hypothesis: tensor([154.0345, 185.0766, 175.8899, 198.5373, 141.2257]) Cost: 5.615090\n",
            "Epoch  158/200 hypothesis: tensor([154.0343, 185.0764, 175.8909, 198.5367, 141.2257]) Cost: 5.612669\n",
            "Epoch  159/200 hypothesis: tensor([154.0342, 185.0761, 175.8920, 198.5362, 141.2258]) Cost: 5.610264\n",
            "Epoch  160/200 hypothesis: tensor([154.0340, 185.0758, 175.8930, 198.5356, 141.2258]) Cost: 5.607846\n",
            "Epoch  161/200 hypothesis: tensor([154.0339, 185.0755, 175.8940, 198.5350, 141.2259]) Cost: 5.605430\n",
            "Epoch  162/200 hypothesis: tensor([154.0337, 185.0753, 175.8950, 198.5344, 141.2260]) Cost: 5.603012\n",
            "Epoch  163/200 hypothesis: tensor([154.0336, 185.0750, 175.8960, 198.5339, 141.2260]) Cost: 5.600634\n",
            "Epoch  164/200 hypothesis: tensor([154.0334, 185.0747, 175.8970, 198.5333, 141.2261]) Cost: 5.598229\n",
            "Epoch  165/200 hypothesis: tensor([154.0332, 185.0744, 175.8981, 198.5327, 141.2262]) Cost: 5.595800\n",
            "Epoch  166/200 hypothesis: tensor([154.0331, 185.0741, 175.8991, 198.5322, 141.2262]) Cost: 5.593424\n",
            "Epoch  167/200 hypothesis: tensor([154.0329, 185.0739, 175.9001, 198.5316, 141.2263]) Cost: 5.590985\n",
            "Epoch  168/200 hypothesis: tensor([154.0328, 185.0736, 175.9011, 198.5310, 141.2263]) Cost: 5.588626\n",
            "Epoch  169/200 hypothesis: tensor([154.0326, 185.0733, 175.9021, 198.5305, 141.2264]) Cost: 5.586225\n",
            "Epoch  170/200 hypothesis: tensor([154.0325, 185.0731, 175.9032, 198.5299, 141.2265]) Cost: 5.583795\n",
            "Epoch  171/200 hypothesis: tensor([154.0323, 185.0728, 175.9042, 198.5293, 141.2265]) Cost: 5.581409\n",
            "Epoch  172/200 hypothesis: tensor([154.0322, 185.0725, 175.9052, 198.5287, 141.2266]) Cost: 5.579025\n",
            "Epoch  173/200 hypothesis: tensor([154.0320, 185.0722, 175.9062, 198.5282, 141.2267]) Cost: 5.576617\n",
            "Epoch  174/200 hypothesis: tensor([154.0319, 185.0720, 175.9072, 198.5276, 141.2267]) Cost: 5.574210\n",
            "Epoch  175/200 hypothesis: tensor([154.0317, 185.0717, 175.9082, 198.5270, 141.2268]) Cost: 5.571843\n",
            "Epoch  176/200 hypothesis: tensor([154.0316, 185.0714, 175.9093, 198.5265, 141.2269]) Cost: 5.569420\n",
            "Epoch  177/200 hypothesis: tensor([154.0314, 185.0711, 175.9103, 198.5259, 141.2269]) Cost: 5.567038\n",
            "Epoch  178/200 hypothesis: tensor([154.0312, 185.0709, 175.9113, 198.5253, 141.2270]) Cost: 5.564670\n",
            "Epoch  179/200 hypothesis: tensor([154.0311, 185.0706, 175.9123, 198.5248, 141.2271]) Cost: 5.562275\n",
            "Epoch  180/200 hypothesis: tensor([154.0309, 185.0703, 175.9133, 198.5242, 141.2271]) Cost: 5.559882\n",
            "Epoch  181/200 hypothesis: tensor([154.0308, 185.0701, 175.9143, 198.5236, 141.2272]) Cost: 5.557498\n",
            "Epoch  182/200 hypothesis: tensor([154.0306, 185.0698, 175.9153, 198.5231, 141.2272]) Cost: 5.555122\n",
            "Epoch  183/200 hypothesis: tensor([154.0304, 185.0695, 175.9164, 198.5225, 141.2273]) Cost: 5.552704\n",
            "Epoch  184/200 hypothesis: tensor([154.0303, 185.0692, 175.9174, 198.5219, 141.2274]) Cost: 5.550362\n",
            "Epoch  185/200 hypothesis: tensor([154.0301, 185.0690, 175.9184, 198.5213, 141.2274]) Cost: 5.547971\n",
            "Epoch  186/200 hypothesis: tensor([154.0300, 185.0687, 175.9194, 198.5208, 141.2275]) Cost: 5.545587\n",
            "Epoch  187/200 hypothesis: tensor([154.0298, 185.0684, 175.9204, 198.5202, 141.2276]) Cost: 5.543214\n",
            "Epoch  188/200 hypothesis: tensor([154.0297, 185.0681, 175.9214, 198.5196, 141.2276]) Cost: 5.540811\n",
            "Epoch  189/200 hypothesis: tensor([154.0295, 185.0679, 175.9224, 198.5191, 141.2277]) Cost: 5.538465\n",
            "Epoch  190/200 hypothesis: tensor([154.0294, 185.0676, 175.9234, 198.5185, 141.2278]) Cost: 5.536077\n",
            "Epoch  191/200 hypothesis: tensor([154.0292, 185.0673, 175.9245, 198.5179, 141.2278]) Cost: 5.533695\n",
            "Epoch  192/200 hypothesis: tensor([154.0291, 185.0671, 175.9255, 198.5174, 141.2279]) Cost: 5.531332\n",
            "Epoch  193/200 hypothesis: tensor([154.0289, 185.0668, 175.9265, 198.5168, 141.2280]) Cost: 5.528962\n",
            "Epoch  194/200 hypothesis: tensor([154.0287, 185.0665, 175.9275, 198.5163, 141.2280]) Cost: 5.526603\n",
            "Epoch  195/200 hypothesis: tensor([154.0286, 185.0663, 175.9285, 198.5157, 141.2281]) Cost: 5.524244\n",
            "Epoch  196/200 hypothesis: tensor([154.0284, 185.0660, 175.9295, 198.5151, 141.2282]) Cost: 5.521854\n",
            "Epoch  197/200 hypothesis: tensor([154.0283, 185.0657, 175.9305, 198.5145, 141.2282]) Cost: 5.519456\n",
            "Epoch  198/200 hypothesis: tensor([154.0281, 185.0654, 175.9315, 198.5140, 141.2283]) Cost: 5.517091\n",
            "Epoch  199/200 hypothesis: tensor([154.0280, 185.0652, 175.9325, 198.5134, 141.2284]) Cost: 5.514731\n",
            "Epoch  200/200 hypothesis: tensor([154.0278, 185.0649, 175.9335, 198.5128, 141.2284]) Cost: 5.512386\n"
          ]
        }
      ]
    }
  ]
}